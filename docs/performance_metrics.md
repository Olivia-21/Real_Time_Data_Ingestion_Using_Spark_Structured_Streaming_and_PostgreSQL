# Performance Metrics Report

**Test Date:** January 29, 2026  
**Test Duration:**  (continuous pipeline operation)

---

## Understanding the Metrics

### What is Throughput?
**Throughput** measures how many events the pipeline can process per unit of time.



### What is Latency?
**Latency** measures how long each individual event takes from creation to database storage.


---

## Summary

| Metric | Value | Explanation |
|--------|-------|-------------|
| **Total Events Ingested** | 9,040 | Total events stored in PostgreSQL |
| **Throughput (average)** | 18.64 events/min | Average over entire test (includes downtime) |
| **Throughput (real-time)** | 37 events/min | Actual rate when pipeline is running |
| **Average Latency** | 150.33 seconds | Time from event creation to database |
| **Duplicates** | 0 | 100% data integrity |
| **View/Purchase Ratio** | 70.6% / 29.4% | Matches expected configuration |

---

## Detailed Metrics

### 1. Throughput

| Metric | Value | Notes |
|--------|-------|-------|
| Real-time throughput | 37 events/min | Measured over last 10 minutes |
| Historical average | 18.64 events/min | Lower due to pipeline downtime |
| Theoretical maximum | 120 events/min | 10 events Ã— 12 batches/min |

**Why is actual lower than theoretical?**
- Pipeline was stopped/restarted multiple times during development
- Spark trigger interval (10s) adds processing overhead
- Docker container scheduling affects timing

### 2. Latency

| Metric | Value | Notes |
|--------|-------|-------|
| Average latency | 150.33s | High due to downtime accumulation |
| Maximum latency | 16,464.70s | Events waited ~4.5 hours during long downtime |
| Expected (continuous) | < 15s | Normal operation latency |

**Why is latency high?**
```
Event created at 10:00:00
Pipeline was OFF (rebuilding Docker)
Pipeline restarted at 10:02:30
Event processed at 10:02:30

Latency = 150 seconds (accumulated wait time)
```

### 3. Event Distribution

| Event Type | Count | Percentage |
|------------|-------|------------|
| View | 6,382 | 70.6% |
| Purchase | 2,658 | 29.4% |
| **Total** | **9,040** | **100%** |

Matches the 70/30 ratio configured in `settings.yaml` (purchase_probability: 0.3)

### 4. Data Integrity

| Check | Result | Status |
|-------|--------|--------|
| Duplicate events | 0 | PASS |
| Unique event IDs | 9,040 | PASS |
| Data loss | 0% | PASS |

---

## SQL Queries Used

```sql
-- Total events and throughput (average)
SELECT 
    COUNT(*) AS total_events,
    ROUND(COUNT(*)::numeric / NULLIF(EXTRACT(EPOCH FROM (MAX(ingested_at) - MIN(ingested_at))), 0) * 60, 2) AS events_per_minute
FROM user_events;

-- Real-time throughput (last 10 minutes)
SELECT 
    COUNT(*) AS events,
    ROUND(COUNT(*)::numeric / 10, 2) AS events_per_minute
FROM user_events 
WHERE ingested_at > NOW() - INTERVAL '10 minutes';

-- Latency
SELECT 
    ROUND(AVG(EXTRACT(EPOCH FROM (ingested_at - event_timestamp)))::numeric, 2) AS avg_latency_seconds,
    ROUND(MAX(EXTRACT(EPOCH FROM (ingested_at - event_timestamp)))::numeric, 2) AS max_latency_seconds
FROM user_events;

-- Event distribution
SELECT event_type, COUNT(*) AS count FROM user_events GROUP BY event_type;

-- Duplicate check
SELECT COUNT(*) - COUNT(DISTINCT event_id) AS duplicates FROM user_events;
```

---

## Configuration Used

```yaml
generator:
  event_size: 10           # Events per CSV file
  interval_seconds: 5      # 5 seconds between batches

spark:
  trigger_interval: 10 seconds
  max_files_per_trigger: 5
```

---

## Conclusions

1. **Data Integrity: EXCELLENT** - Zero duplicates confirms deterministic ID + ON CONFLICT strategy works.

2. **Throughput: ACCEPTABLE** - 37 events/min real-time is reasonable for this configuration.

3. **Latency: CONTEXT-DEPENDENT** - High average due to downtime, not a performance issue.

4. **Event Distribution: AS EXPECTED** - 70/30 ratio matches configuration.

---

## Recommendations for Production

| Optimization | Impact |
|--------------|--------|
| Reduce `trigger_interval` to 5s | Lower latency |
| Increase `event_size` to 100 | Higher throughput |
| Add monitoring/alerting | Detect issues faster |
| Implement dead-letter queue | Handle bad records |
