# Configuration for Real-Time Spark Streaming Pipeline

# Data Generator Settings
generator:
  output_path: /data/incoming
  batch_size: 100                    # Events per CSV file
  interval_seconds: 5               # Time between batch generations
  event_types:
    - view
    - purchase
  purchase_probability: 0.3         # 30% chance of purchase vs view

# Product Catalog (for realistic data generation)
products:
  categories:
    - Electronics
    - Clothing
    - Home & Garden
    - Sports
    - Books
    - Toys
    - Beauty
    - Food & Grocery

# Spark Streaming Settings
spark:
  app_name: EcommerceEventStreaming
  master: local[*]
  trigger_interval: 10 seconds      # Processing trigger interval
  checkpoint_path: /data/checkpoint
  input_path: /data/incoming
  max_files_per_trigger: 5          # Limit files processed per batch

# PostgreSQL Settings
postgres:
  host: postgres
  port: 5432
  database: ecommerce_events
  user: postgres
  password: postgres123
  table: user_events

# Logging Settings
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  max_bytes: 10485760               # 10MB
  backup_count: 5

# Data Validation Rules
validation:
  min_price: 0.0
  max_future_timestamp_seconds: 60  # Allow up to 60 seconds in future for clock drift
  required_fields:
    - event_id
    - user_id
    - product_id
    - product_name
    - product_category
    - event_type
    - event_timestamp

# Retry Settings for Database Operations
retry:
  max_attempts: 3
  initial_delay_seconds: 1
  max_delay_seconds: 30
  exponential_base: 2
